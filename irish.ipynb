{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTa6USgdwd26fDyZZGGV63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-2/blob/AG_Murray/irish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sPoargROh-CD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "import pickle\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJvphPGY7b3",
        "outputId": "5596c66e-2e85-4e30-d4f4-e6018730b2f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    with open(\"train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "\n",
        "        cur_sent = []\n",
        "        all_sents = []\n",
        "\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            if line[0][0:2].strip() == 'N':\n",
        "                line[0] = 'N'\n",
        "\n",
        "            if line[0] == \"<S>\":\n",
        "                if len(cur_sent) >= 1:\n",
        "                    all_sents.append(cur_sent)\n",
        "                cur_sent = []\n",
        "                continue\n",
        "\n",
        "            cur_sent.append((line[0], line[1]))\n",
        "\n",
        "    return all_sents"
      ],
      "metadata": {
        "id": "uNyCz3x0i2DH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_num(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    label = le.fit_transform(data)\n",
        "\n",
        "    return label"
      ],
      "metadata": {
        "id": "hyVCOhZzsj7P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def defeature(data):\n",
        "    unfeatured = []\n",
        "\n",
        "    for dict in data:\n",
        "        unfeatured.append(dict['word'])\n",
        "\n",
        "    return unfeatured"
      ],
      "metadata": {
        "id": "brkGW1HtsmZ1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xdUPk8WGiX36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(x, y):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(1,), activation=\"sigmoid\"))\n",
        "    model.add(Dense(128, activation=\"sigmoid\"))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    model.add(Dense(1, activation=\"softmax\"))\n",
        "\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    #stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    x = defeature(x)\n",
        "\n",
        "    x = string_to_num(x)\n",
        "    y = string_to_num(y)\n",
        "\n",
        "    x = np.asarray(x)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.01)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
        "    H = model.fit(x, y, epochs=100, batch_size=100) #, callbacks=[stop_early])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vCoRohe7iDg5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "  x_test = defeature(x_test)\n",
        "  x_test = string_to_num(x_test)\n",
        "  y_test = string_to_num(y_test)\n",
        "  return model.evaluate(x_test[:5000], y_test[:5000], return_dict=True)"
      ],
      "metadata": {
        "id": "jPe1yG7k3UlV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below work partially https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "H2fNKCqniaML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree(X, y):\n",
        "    clf = Pipeline([\n",
        "        ('vectorizer', DictVectorizer(sparse=False)),\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    print(\"Training Started\")\n",
        "\n",
        "    # Custom work below\n",
        "    all_clfs = []\n",
        "\n",
        "    for i in tqdm(range(0, 115), desc=\"Training\"):\n",
        "        cur_clf = clf\n",
        "        cur_clf.fit(X[(i * 31649): ((i + 1) * 31649)], y[(i * 31649): ((i + 1) * 31649)])  # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
        "\n",
        "        all_clfs.append(cur_clf)\n",
        "\n",
        "        file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "        file_loc_2 = '/gdrive/MyDrive/Colab_Notebooks/NLP/vector' + str(i) + \".pkl\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "        with open(file_loc, 'wb') as pickle_file:\n",
        "          pickle.dump(cur_clf, pickle_file)\n",
        "\n",
        "        # cur_clf.named_steps['estimator'].model = None\n",
        "\n",
        "        # joblib.dump(cur_clf, file_loc_2)\n",
        "\n",
        "    print('Training completed')\n",
        "\n",
        "    return all_clfs"
      ],
      "metadata": {
        "id": "8Fk2d3XkiITc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4KX-6M76iggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_score(all_clfs, x_test, y_test):\n",
        "    all_scores = []\n",
        "\n",
        "    for clf in tqdm(all_clfs, desc=\"evaluating...\"):\n",
        "      temp_scores = []\n",
        "      for i in range(0,50):\n",
        "        temp_scores.append(clf.score(x_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))], y_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))]))\n",
        "\n",
        "        all_scores.append(sum(temp_scores)/ len(temp_scores))\n",
        "\n",
        "    total_acc = 0\n",
        "\n",
        "    for score in all_scores:\n",
        "        total_acc += score\n",
        "\n",
        "    print(\"Accuracy:\", total_acc / len(all_scores))"
      ],
      "metadata": {
        "id": "X3xUNpkIiKnW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "r3j2axkJiiPS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "zt5W_NP_iNRE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "LYe6tgA6ijKY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ],
      "metadata": {
        "id": "AbjAsK4biPi8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "ZzzBYcRJikGI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "\n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged), index))\n",
        "            y.append(tagged[index][1])\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "IhqgzoFyiSEg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "  all_clfs = []\n",
        "\n",
        "  for i in range(1, 115):\n",
        "    file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "    with open(file_loc, 'rb') as pickle_file:\n",
        "      cur_clf = pickle.load(pickle_file)\n",
        "\n",
        "      all_clfs.append(cur_clf)\n",
        "\n",
        "  return all_clfs\n",
        "\n"
      ],
      "metadata": {
        "id": "_i6pbwlXun9i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below based on code provided by Dr. Scannell"
      ],
      "metadata": {
        "id": "PL1fYqeXo4jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_uni(train, test):\n",
        "    uni_tag = nltk.UnigramTagger(train)\n",
        "    acc = uni_tag.accuracy(test)\n",
        "\n",
        "    print(\"Unigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "83zvY8KEo3wE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rule_tag():\n",
        "    patterns = [\n",
        "        (r'a .*', 'H'),\n",
        "        (r'an .*', 'H'),\n",
        "        (r'mo .*', 'H'),\n",
        "        (r'do .*', 'H')\n",
        "    ]\n",
        "\n",
        "    rule_tagger = nltk.RegexpTagger(patterns)\n",
        "\n",
        "    return rule_tagger"
      ],
      "metadata": {
        "id": "JltwhRlCJYlb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below based on code provided by Dr. Scannell"
      ],
      "metadata": {
        "id": "xFTTD2tDwCKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_bi(train, test):\n",
        "    rule_tagger = gen_rule_tag()\n",
        "\n",
        "    print(train[:5])\n",
        "    print(len(train[0]))\n",
        "\n",
        "    bigrams = gen_grams(train, 1)\n",
        "    \n",
        "    print(bigrams[0:5])\n",
        "    print(len(bigrams[0]))\n",
        "\n",
        "    bitag = nltk.UnigramTagger(bigrams, backoff=rule_tagger)\n",
        "    acc = bitag.accuracy(test)\n",
        "\n",
        "    print(\"Bigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "w0oukDklv9Sb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_tri(train, test):\n",
        "    tritag = nltk.TrigramTagger(train)\n",
        "\n",
        "    acc = tritag.accuracy(test)\n",
        "\n",
        "    print(\"Trigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "C6RuwcOd0HG0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_list(data):\n",
        "    all_data = []\n",
        "\n",
        "    for entry in data:\n",
        "      for tup in entry:\n",
        "        all_data.append(tup)\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "oKokSn7QtTYW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_grams(data, n = 1):\n",
        "    all_grams = []\n",
        "    \n",
        "    for sent in data:\n",
        "      cur_grams = [(' '.join([sent[i][0], sent[i+1][0]]), sent[i+1][1]) for i in range(len(sent)-1)]\n",
        "\n",
        "      cur_grams.append(sent[len(sent)-1])\n",
        "\n",
        "      all_grams.append(cur_grams)\n",
        "\n",
        "    return all_grams"
      ],
      "metadata": {
        "id": "_KTfWs01HGaC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anything():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    print('Splitting data')\n",
        "\n",
        "  # from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    training_sentences = data[:cutoff]\n",
        "    test_sentences = data[cutoff:]\n",
        "\n",
        "    print('Transforming training data')\n",
        "\n",
        "    X, y = transform_to_dataset(training_sentences)\n",
        "\n",
        "    del data\n",
        "\n",
        "    print('Transforming test data')\n",
        "\n",
        "    x_test, y_test = transform_to_dataset(test_sentences)\n",
        "\n",
        "    print('Loading models')\n",
        "\n",
        "  # Original\n",
        "    #all_clfs = dec_tree(X, y) # Training Decision Tree Models\n",
        "    #all_clfs = load_models() # Loading models\n",
        "    #acc_score(all_clfs, x_test, y_test) # Calculating accuracy\n",
        "\n",
        "    # model = seq_model(X, y) # Train sequential model\n",
        "    # print(eval_model(model, x_test, y_test)) # Eval sequential model\n",
        "\n",
        "    # model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/seq_model.h5')\n",
        "\n",
        "    HMM_uni(training_sentences, test_sentences)\n",
        "\n",
        "    HMM_bi(training_sentences, test_sentences)\n",
        "\n",
        "    HMM_tri(training_sentences, test_sentences)\n",
        "\n",
        "    del training_sentences, test_sentences"
      ],
      "metadata": {
        "id": "vJ1CYCjSiBee"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anything()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "ezpgqmmqiukB",
        "outputId": "7b85715d-1bc0-46b1-dd13-bdbbe4ef294a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 5057059it [00:26, 188745.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data\n",
            "Transforming training data\n",
            "Transforming test data\n",
            "Loading models\n",
            "Unigram tagger accuracy:  0.8907771596737659\n",
            "[[('ansin', 'N'), (')', 'N'), ('tá', 'N'), ('níos', 'N'), ('lú', 'N'), ('gaeilge', 'N'), ('ag', 'N'), ('na', 'N'), ('gardaí', 'N'), ('ná', 'N'), ('bí', 'S'), ('ariamh', 'N'), ('ainneoin', 'N'), ('na', 'N'), ('cearta', 'U'), ('.', 'N'), ('níl', 'N'), ('sé', 'N'), ('ach', 'N'), ('roinnt', 'N'), ('seachtainí', 'N'), ('ó', 'N'), ('sin', 'S'), ('a', 'N'), ('tógadh', 'N'), ('fear', 'N'), ('bocht', 'N'), ('a', 'N'), ('tug', 'S'), ('ainm', 'N'), ('gaeilge', 'N'), ('dóibh', 'N'), ('.', 'N')], [('socraíodh', 'N'), ('go', 'N'), ('raibh', 'N'), ('gá', 'N'), ('lena', 'N'), ('leithéid', 'N'), (',', 'N'), ('mar', 'N'), ('go', 'N'), ('bíonn', 'U'), ('na', 'N')], [('tá', 'N'), ('an', 'N'), ('córas', 'N'), ('bainistíochta', 'N'), ('tar', 'N'), ('éis', 'N'), ('freastal', 'N'), ('go', 'N'), ('maith', 'N'), ('ar', 'N'), ('rialtas', 'N'), ('áitiúil', 'N'), ('na', 'N'), ('éireann', 'H'), ('agus', 'N'), ('leanfaidh', 'N'), ('údaráis', 'N'), ('áitiúla', 'N'), ('ar', 'N'), ('aghaidh', 'N'), ('ag', 'N'), ('brath', 'N'), ('ar', 'N'), ('bainisteoirí', 'S'), ('gairmiúla', 'N'), ('le', 'N'), ('treoir', 'N'), ('agus', 'N'), ('riarachán', 'N'), ('gairmiúil', 'N'), ('neamhchlaon', 'N'), ('a', 'N'), ('soláthar', 'S'), ('.', 'N')], [('john', 'N'), (\"o'donovanba\", 'N'), ('luath', 'N'), ('a', 'N'), (\"d'éirigh\", 'N'), ('larcom', 'N'), ('as', 'N'), ('a', 'N'), ('beith', 'S'), ('ag', 'N'), ('dul', 'N'), ('don', 'N'), ('gaeilge', 'S'), ('agus', 'N'), ('a', 'N'), ('roghnaigh', 'N'), ('réiteach', 'N'), ('eile', 'N'), ('áfach', 'N'), ('–', 'N'), ('fostú', 'N'), ('scoláirí', 'N'), ('gaeilge', 'N'), ('nó', 'N'), ('‘', 'N'), ('toponymic', 'N'), ('field', 'N'), ('workers', 'N'), ('’', 'N'), ('a', 'N'), ('beadh', 'S'), ('mar', 'N'), ('saothar', 'S'), ('acu', 'N'), ('cun', 'S'), ('an', 'N'), ('bunchiall', 'N'), ('teangeolaíochta', 'N'), ('le', 'N'), ('logainmneacha', 'N'), ('a', 'N'), ('bunú', 'S'), ('is', 'N'), ('a', 'N'), ('socrú', 'S'), ('.', 'N')], [('teangeolaí', 'N'), ('agus', 'N'), ('tráchtaire', 'N'), ('polaitiúil', 'N'), ('den', 'N'), ('céad', 'S'), ('scoth', 'N'), (',', 'N'), ('seo', 'N'), ('fear', 'N'), ('gur', 'N'), ('fiú', 'N'), ('éisteacht', 'N'), ('leis', 'N'), ('.', 'N')]]\n",
            "33\n",
            "[[('ansin)', 'N'), (')tá', 'N'), ('táníos', 'N'), ('níoslú', 'N'), ('lúgaeilge', 'N'), ('gaeilgeag', 'N'), ('agna', 'N'), ('nagardaí', 'N'), ('gardaíná', 'N'), ('nábí', 'S'), ('bíariamh', 'N'), ('ariamhainneoin', 'N'), ('ainneoinna', 'N'), ('nacearta', 'U'), ('cearta.', 'N'), ('.níl', 'N'), ('nílsé', 'N'), ('séach', 'N'), ('achroinnt', 'N'), ('roinntseachtainí', 'N'), ('seachtainíó', 'N'), ('ósin', 'S'), ('sina', 'N'), ('atógadh', 'N'), ('tógadhfear', 'N'), ('fearbocht', 'N'), ('bochta', 'N'), ('atug', 'S'), ('tugainm', 'N'), ('ainmgaeilge', 'N'), ('gaeilgedóibh', 'N'), ('dóibh.', 'N'), ('.', 'N')], [('socraíodhgo', 'N'), ('goraibh', 'N'), ('raibhgá', 'N'), ('gálena', 'N'), ('lenaleithéid', 'N'), ('leithéid,', 'N'), (',mar', 'N'), ('margo', 'N'), ('gobíonn', 'U'), ('bíonnna', 'N'), ('na', 'N')], [('táan', 'N'), ('ancóras', 'N'), ('córasbainistíochta', 'N'), ('bainistíochtatar', 'N'), ('taréis', 'N'), ('éisfreastal', 'N'), ('freastalgo', 'N'), ('gomaith', 'N'), ('maithar', 'N'), ('arrialtas', 'N'), ('rialtasáitiúil', 'N'), ('áitiúilna', 'N'), ('naéireann', 'H'), ('éireannagus', 'N'), ('agusleanfaidh', 'N'), ('leanfaidhúdaráis', 'N'), ('údaráisáitiúla', 'N'), ('áitiúlaar', 'N'), ('araghaidh', 'N'), ('aghaidhag', 'N'), ('agbrath', 'N'), ('brathar', 'N'), ('arbainisteoirí', 'S'), ('bainisteoirígairmiúla', 'N'), ('gairmiúlale', 'N'), ('letreoir', 'N'), ('treoiragus', 'N'), ('agusriarachán', 'N'), ('riarachángairmiúil', 'N'), ('gairmiúilneamhchlaon', 'N'), ('neamhchlaona', 'N'), ('asoláthar', 'S'), ('soláthar.', 'N'), ('.', 'N')], [(\"johno'donovanba\", 'N'), (\"o'donovanbaluath\", 'N'), ('luatha', 'N'), (\"ad'éirigh\", 'N'), (\"d'éirighlarcom\", 'N'), ('larcomas', 'N'), ('asa', 'N'), ('abeith', 'S'), ('beithag', 'N'), ('agdul', 'N'), ('duldon', 'N'), ('dongaeilge', 'S'), ('gaeilgeagus', 'N'), ('agusa', 'N'), ('aroghnaigh', 'N'), ('roghnaighréiteach', 'N'), ('réiteacheile', 'N'), ('eileáfach', 'N'), ('áfach–', 'N'), ('–fostú', 'N'), ('fostúscoláirí', 'N'), ('scoláirígaeilge', 'N'), ('gaeilgenó', 'N'), ('nó‘', 'N'), ('‘toponymic', 'N'), ('toponymicfield', 'N'), ('fieldworkers', 'N'), ('workers’', 'N'), ('’a', 'N'), ('abeadh', 'S'), ('beadhmar', 'N'), ('marsaothar', 'S'), ('saotharacu', 'N'), ('acucun', 'S'), ('cunan', 'N'), ('anbunchiall', 'N'), ('bunchiallteangeolaíochta', 'N'), ('teangeolaíochtale', 'N'), ('lelogainmneacha', 'N'), ('logainmneachaa', 'N'), ('abunú', 'S'), ('bunúis', 'N'), ('isa', 'N'), ('asocrú', 'S'), ('socrú.', 'N'), ('.', 'N')], [('teangeolaíagus', 'N'), ('agustráchtaire', 'N'), ('tráchtairepolaitiúil', 'N'), ('polaitiúilden', 'N'), ('dencéad', 'S'), ('céadscoth', 'N'), ('scoth,', 'N'), (',seo', 'N'), ('seofear', 'N'), ('feargur', 'N'), ('gurfiú', 'N'), ('fiúéisteacht', 'N'), ('éisteachtleis', 'N'), ('leis.', 'N'), ('.', 'N')]]\n",
            "33\n",
            "Bigram tagger accuracy:  0.7514071656795351\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-25bfa6a00581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-7177862c181a>\u001b[0m in \u001b[0;36manything\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mHMM_bi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mHMM_tri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtraining_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-0ab0fadc151c>\u001b[0m in \u001b[0;36mHMM_tri\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtritag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrigramTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtritag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trigram tagger accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/api.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, gold)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muntag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mgold_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/api.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use accuracy(gold) instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use accuracy(gold) instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_to_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}