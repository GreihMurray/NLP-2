{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMGXOf1F9zJIq9fkKWYjxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-2/blob/AG_Murray/irish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07d0Yf53nAZR",
        "outputId": "233f5da3-5a42-49fc-d0fd-c48fb0e07738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPoargROh-CD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "import pickle\n",
        "import nltk\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJvphPGY7b3",
        "outputId": "cb41517d-1ec0-4426-afc2-42708fc52439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    with open(\"train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "\n",
        "        cur_sent = []\n",
        "        all_sents = []\n",
        "\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            if line[0][0:2].strip() == 'N':\n",
        "                line[0] = 'N'\n",
        "\n",
        "            if line[0] == \"<S>\":\n",
        "                if len(cur_sent) >= 1:\n",
        "                    all_sents.append(cur_sent)\n",
        "                cur_sent = []\n",
        "                continue\n",
        "\n",
        "            cur_sent.append((line[0], line[1]))\n",
        "\n",
        "    return all_sents"
      ],
      "metadata": {
        "id": "uNyCz3x0i2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_num(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    label = le.fit_transform(data)\n",
        "\n",
        "    return label"
      ],
      "metadata": {
        "id": "hyVCOhZzsj7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def defeature(data):\n",
        "    unfeatured = []\n",
        "\n",
        "    for dict in data:\n",
        "        unfeatured.append(dict['word'])\n",
        "\n",
        "    return unfeatured"
      ],
      "metadata": {
        "id": "brkGW1HtsmZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(data):\n",
        "  vocab = list(set([w for sent in data for (w,t) in sent]))\n",
        "  vocab.append('<PAD>')\n",
        "  print(len(vocab))\n",
        "  tags = list(set([t for sent in data for (w,t) in sent]))\n",
        "  tags.append('<PAD>')\n",
        "\n",
        "  return vocab, tags"
      ],
      "metadata": {
        "id": "odcDq3EWJoh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(vocab, tags, data):\n",
        "  max_len = 60\n",
        "  word2index = {w: i for i, w in enumerate(vocab)}\n",
        "  tag2index = {t: i for i, t in enumerate(tags)}\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab)-1)\n",
        "\n",
        "  onehot_y = [[tag2index[w[1]] for w in s] for s in data]\n",
        "  y = pad_sequences(maxlen=max_len, sequences=onehot_y, padding=\"post\", value=tag2index[\"<PAD>\"])\n",
        "  y = to_categorical(y, num_classes=len(tags))\n",
        "\n",
        "  return X, y, max_len"
      ],
      "metadata": {
        "id": "IL9ZvqUxJ4SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on code provided by Dr. Scannell in BiLTSM notebook"
      ],
      "metadata": {
        "id": "xdUPk8WGiX36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(data):\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=50, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
        "    model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n",
        "    model.compile(optimizer=\"adam\", loss=\"poisson\", metrics=[\"accuracy\"])\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.01)\n",
        "    history = model.fit(X_train, y_train, batch_size=512, epochs=50, validation_split=0.2, verbose=1, callbacks=stop_early)\n",
        "\n",
        "    return model, X_test, y_test"
      ],
      "metadata": {
        "id": "vCoRohe7iDg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "  #x_test = defeature(x_test)\n",
        "  #_test = string_to_num(x_test)\n",
        "  #y_test = string_to_num(y_test)\n",
        "  return model.evaluate(x_test[:5000], y_test[:5000], return_dict=True)"
      ],
      "metadata": {
        "id": "jPe1yG7k3UlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below work partially https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "H2fNKCqniaML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree(X, y):\n",
        "    clf = Pipeline([\n",
        "        ('vectorizer', DictVectorizer(sparse=False)),\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    print(\"Training Started\")\n",
        "\n",
        "    # Custom work below\n",
        "    all_clfs = []\n",
        "\n",
        "    for i in tqdm(range(0, 115), desc=\"Training\"):\n",
        "        cur_clf = clf\n",
        "        cur_clf.fit(X[(i * 31649): ((i + 1) * 31649)], y[(i * 31649): ((i + 1) * 31649)])  # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
        "\n",
        "        all_clfs.append(cur_clf)\n",
        "\n",
        "        file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "        file_loc_2 = '/gdrive/MyDrive/Colab_Notebooks/NLP/vector' + str(i) + \".pkl\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "        with open(file_loc, 'wb') as pickle_file:\n",
        "          pickle.dump(cur_clf, pickle_file)\n",
        "\n",
        "        # cur_clf.named_steps['estimator'].model = None\n",
        "\n",
        "        # joblib.dump(cur_clf, file_loc_2)\n",
        "\n",
        "    print('Training completed')\n",
        "\n",
        "    return all_clfs"
      ],
      "metadata": {
        "id": "8Fk2d3XkiITc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4KX-6M76iggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_score(all_clfs, x_test, y_test):\n",
        "    all_scores = []\n",
        "\n",
        "    for clf in tqdm(all_clfs, desc=\"evaluating...\"):\n",
        "      temp_scores = []\n",
        "      for i in range(0,50):\n",
        "        temp_scores.append(clf.score(x_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))], y_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))]))\n",
        "\n",
        "        all_scores.append(sum(temp_scores)/ len(temp_scores))\n",
        "\n",
        "    total_acc = 0\n",
        "\n",
        "    for score in all_scores:\n",
        "        total_acc += score\n",
        "\n",
        "    print(\"Accuracy:\", total_acc / len(all_scores))"
      ],
      "metadata": {
        "id": "X3xUNpkIiKnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "r3j2axkJiiPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "zt5W_NP_iNRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "LYe6tgA6ijKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ],
      "metadata": {
        "id": "AbjAsK4biPi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "ZzzBYcRJikGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "\n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged), index))\n",
        "            y.append(tagged[index][1])\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "IhqgzoFyiSEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "  all_clfs = []\n",
        "\n",
        "  for i in range(1, 115):\n",
        "    file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "    with open(file_loc, 'rb') as pickle_file:\n",
        "      cur_clf = pickle.load(pickle_file)\n",
        "\n",
        "      all_clfs.append(cur_clf)\n",
        "\n",
        "  return all_clfs\n",
        "\n"
      ],
      "metadata": {
        "id": "_i6pbwlXun9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_seq_model(filename):\n",
        "    model = load_model(filename)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ptzyXafyq8ZM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below based on code provided by Dr. Scannell"
      ],
      "metadata": {
        "id": "PL1fYqeXo4jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_uni(train, test):\n",
        "    uni_tag = nltk.UnigramTagger(train)\n",
        "    acc = uni_tag.accuracy(test)\n",
        "\n",
        "    print(\"Unigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "83zvY8KEo3wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rule_tag():\n",
        "    patterns = [\n",
        "        (r'a .*', 'H'),\n",
        "        (r'an .*', 'H'),\n",
        "        (r'mo .*', 'H'),\n",
        "        (r'do .*', 'H'),\n",
        "        (r'don .*', 'H'),\n",
        "        (r'den .*', 'H'),\n",
        "        (r'an d.*', 'N'),\n",
        "        (r'an t.*', 'N'),\n",
        "        (r'faoi .*', 'N')\n",
        "    ]\n",
        "\n",
        "    rule_tagger = nltk.RegexpTagger(patterns)\n",
        "\n",
        "    return rule_tagger"
      ],
      "metadata": {
        "id": "JltwhRlCJYlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below based on code provided by Dr. Scannell"
      ],
      "metadata": {
        "id": "xFTTD2tDwCKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_bi(train, test):\n",
        "    rule_tagger = gen_rule_tag()\n",
        "\n",
        "    bigrams = gen_grams(train, 1)\n",
        "    test = gen_grams(test)\n",
        "\n",
        "    bitag = nltk.UnigramTagger(bigrams, backoff=rule_tagger)\n",
        "    acc = bitag.accuracy(test)\n",
        "\n",
        "    print(\"Bigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "w0oukDklv9Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_tri(train, test):\n",
        "    tritag = nltk.TrigramTagger(train)\n",
        "\n",
        "    acc = tritag.accuracy(test)\n",
        "\n",
        "    print(\"Trigram tagger accuracy: \", acc)"
      ],
      "metadata": {
        "id": "C6RuwcOd0HG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_list(data):\n",
        "    all_data = []\n",
        "\n",
        "    for entry in data:\n",
        "      for tup in entry:\n",
        "        all_data.append(tup)\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "oKokSn7QtTYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_grams(data, n = 1):\n",
        "    all_grams = []\n",
        "    \n",
        "    for sent in data:\n",
        "      cur_grams = [(' '.join([sent[i][0], sent[i+1][0]]), sent[i+1][1]) for i in range(len(sent)-1)]\n",
        "\n",
        "      cur_grams.append(sent[len(sent)-1])\n",
        "\n",
        "      all_grams.append(cur_grams)\n",
        "\n",
        "    return all_grams"
      ],
      "metadata": {
        "id": "_KTfWs01HGaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(data):\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "LwAszi9_rhqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_whole_seq(data):\n",
        "  #'/content/gdrive/MyDrive/Colab_Notebooks/NLP/fancy_seq_model.h5'\n",
        "  #'/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson_seq_model.h5'\n",
        "\n",
        "    model = load_seq_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson_seq_model.h5')\n",
        "    x_test, y_test = prep_data(data)\n",
        "\n",
        "    print(eval_model(model, x_test, y_test))"
      ],
      "metadata": {
        "id": "PwMoBgDQ_8kK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anything():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "    print('Splitting data')\n",
        "\n",
        "  # from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    training_sentences = data[:cutoff]\n",
        "    test_sentences = data[cutoff:]\n",
        "\n",
        "  # Original\n",
        "    #all_clfs = dec_tree(X, y) # Training Decision Tree Models\n",
        "    #all_clfs = load_models() # Loading models\n",
        "    #acc_score(all_clfs, x_test, y_test) # Calculating accuracy\n",
        "\n",
        "    # HMM_uni(training_sentences, test_sentences)\n",
        "\n",
        "    # HMM_bi(training_sentences, test_sentences)\n",
        "\n",
        "    # HMM_tri(training_sentences, test_sentences)\n",
        "\n",
        "    #model, x_test, y_test = seq_model(data) # Train sequential model\n",
        "\n",
        "    test_whole_seq(data)\n",
        "\n",
        "    #print(eval_model(model, x_test, y_test)) # Eval sequential model\n",
        "\n",
        "    #model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson_seq_model.h5')\n",
        "\n",
        "    del training_sentences, test_sentences"
      ],
      "metadata": {
        "id": "vJ1CYCjSiBee"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anything()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpgqmmqiukB",
        "outputId": "e9a2ac91-a291-46d0-a2e3-d97d03085eb0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 5057059it [00:05, 874600.50it/s]\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data\n",
            "115827\n",
            "157/157 [==============================] - 5s 29ms/step - loss: 0.0212 - accuracy: 0.9937\n",
            "{'loss': 0.021168744191527367, 'accuracy': 0.9937133193016052}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADAM Poisson Accuracy: 99.371\n",
        "\n",
        "ADAM Cross Entropy Accuracy: 99.370\n",
        "\n"
      ],
      "metadata": {
        "id": "l2bEug_WMoQf"
      }
    }
  ]
}