{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-2/blob/master/irish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "07d0Yf53nAZR"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "sPoargROh-CD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import nltk\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJvphPGY7b3",
        "outputId": "4ca829aa-44aa-42a4-9995-439ad90239a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "uNyCz3x0i2DH"
      },
      "outputs": [],
      "source": [
        "#Original\n",
        "def read_file_to_sents():\n",
        "    i = 1\n",
        "    with open(\"train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "\n",
        "        cur_sent = []\n",
        "        all_sents = []\n",
        "\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            if line[0][0:2].strip() == 'N':\n",
        "                line[0] = 'N'\n",
        "\n",
        "            if line[0] == \"<S>\":\n",
        "                if len(cur_sent) >= 1:\n",
        "                    all_sents.append(cur_sent)\n",
        "                cur_sent = []\n",
        "                continue\n",
        "            cur_sent.append((line[0], line[1]))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    return all_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "hyVCOhZzsj7P"
      },
      "outputs": [],
      "source": [
        "#Original\n",
        "def string_to_num(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    label = le.fit_transform(data)\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "brkGW1HtsmZ1"
      },
      "outputs": [],
      "source": [
        "#Original\n",
        "def defeature(data):\n",
        "    unfeatured = []\n",
        "\n",
        "    for dict in data:\n",
        "        unfeatured.append(dict['word'])\n",
        "\n",
        "    return unfeatured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "odcDq3EWJoh7"
      },
      "outputs": [],
      "source": [
        "#From Challenge 1 https://github.com/GreihMurray/NLP\n",
        "def pad(data):\n",
        "  vocab = list(set([w for sent in data for (w,t) in sent]))\n",
        "  vocab.append('<PAD>')\n",
        "  print(len(vocab))\n",
        "  tags = list(set([t for sent in data for (w,t) in sent]))\n",
        "  tags.append('<PAD>')\n",
        "\n",
        "  return vocab, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "IL9ZvqUxJ4SZ"
      },
      "outputs": [],
      "source": [
        "#From Challenge 1 https://github.com/GreihMurray/NLP\n",
        "def encode(vocab, tags, data):\n",
        "  max_len = 60\n",
        "  word2index = {w: i for i, w in enumerate(vocab)}\n",
        "  tag2index = {t: i for i, t in enumerate(tags)}\n",
        "  onehot = [[word2index[w[0]] for w in s] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len, sequences=onehot, padding=\"post\", value=len(vocab)-1)\n",
        "\n",
        "  onehot_y = [[tag2index[w[1]] for w in s] for s in data]\n",
        "  y = pad_sequences(maxlen=max_len, sequences=onehot_y, padding=\"post\", value=tag2index[\"<PAD>\"])\n",
        "  y = to_categorical(y, num_classes=len(tags))\n",
        "\n",
        "  return X, y, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "vCoRohe7iDg5"
      },
      "outputs": [],
      "source": [
        "#Part original partly from Dr. Scannell\n",
        "def seq_model(data, test_len):\n",
        "  #Original\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    X_train, X_test = x[:(len(data) - test_len)], x[(len(data) - test_len):]\n",
        "    y_train, y_test = y[:(len(data) - test_len)], y[(len(data) - test_len):]\n",
        "  \n",
        "  \n",
        "  # Dr. Scannell\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=50, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.05)))\n",
        "    model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n",
        "    model.compile(optimizer=\"adam\", loss=\"poisson\", metrics=[\"accuracy\"])\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.05)\n",
        "    history = model.fit(X_train, y_train, batch_size=1024, epochs=50, validation_split=0.15, verbose=1, callbacks=stop_early)\n",
        "\n",
        "    return model, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "jPe1yG7k3UlV"
      },
      "outputs": [],
      "source": [
        "#Original\n",
        "def eval_model(model, x_test, y_test):\n",
        "  return model.evaluate(x_test, y_test, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2fNKCqniaML"
      },
      "source": [
        "Below work partially from https://nlpforhackers.io/training-pos-tagger/amp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "8Fk2d3XkiITc"
      },
      "outputs": [],
      "source": [
        "def dec_tree(X, y):\n",
        "    clf = Pipeline([\n",
        "        ('vectorizer', DictVectorizer(sparse=False)),\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    print(\"Training Started\")\n",
        "\n",
        "    # Custom work below\n",
        "    all_clfs = []\n",
        "\n",
        "    for i in tqdm(range(0, 115), desc=\"Training\"):\n",
        "        cur_clf = clf\n",
        "        cur_clf.fit(X[(i * 31649): ((i + 1) * 31649)], y[(i * 31649): ((i + 1) * 31649)])\n",
        "\n",
        "        all_clfs.append(cur_clf)\n",
        "\n",
        "        file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "        file_loc_2 = '/gdrive/MyDrive/Colab_Notebooks/NLP/vector' + str(i) + \".pkl\"\n",
        "\n",
        "\n",
        "        with open(file_loc, 'wb') as pickle_file:\n",
        "          pickle.dump(cur_clf, pickle_file)\n",
        "\n",
        "    print('Training completed')\n",
        "\n",
        "    return all_clfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KX-6M76iggH"
      },
      "source": [
        "Custom work below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "X3xUNpkIiKnW"
      },
      "outputs": [],
      "source": [
        "def acc_score(all_clfs, x_test, y_test):\n",
        "    all_scores = []\n",
        "\n",
        "    for clf in tqdm(all_clfs, desc=\"evaluating...\"):\n",
        "      temp_scores = []\n",
        "      for i in range(0,50):\n",
        "        temp_scores.append(clf.score(x_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))], y_test[int(i * (len(x_test) / 250)):int( (i + 1) * (len(x_test)/250))]))\n",
        "\n",
        "        all_scores.append(sum(temp_scores)/ len(temp_scores))\n",
        "\n",
        "    total_acc = 0\n",
        "\n",
        "    for score in all_scores:\n",
        "        total_acc += score\n",
        "\n",
        "    print(\"Accuracy:\", total_acc / len(all_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "r3j2axkJiiPS"
      },
      "outputs": [],
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "zt5W_NP_iNRE"
      },
      "outputs": [],
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "LYe6tgA6ijKY"
      },
      "outputs": [],
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "AbjAsK4biPi8"
      },
      "outputs": [],
      "source": [
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "ZzzBYcRJikGI"
      },
      "outputs": [],
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "IhqgzoFyiSEg"
      },
      "outputs": [],
      "source": [
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "\n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged), index))\n",
        "            y.append(tagged[index][1])\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom work below"
      ],
      "metadata": {
        "id": "Pxtju0SD9zEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "_i6pbwlXun9i"
      },
      "outputs": [],
      "source": [
        "def load_models():\n",
        "  all_clfs = []\n",
        "\n",
        "  for i in range(1, 115):\n",
        "    file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "    with open(file_loc, 'rb') as pickle_file:\n",
        "      cur_clf = pickle.load(pickle_file)\n",
        "\n",
        "      all_clfs.append(cur_clf)\n",
        "\n",
        "  return all_clfs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom work below"
      ],
      "metadata": {
        "id": "DUHSkop99045"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "ptzyXafyq8ZM"
      },
      "outputs": [],
      "source": [
        "def load_seq_model(filename):\n",
        "    model = load_model(filename)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL1fYqeXo4jn"
      },
      "source": [
        "Below based on code provided by Dr. Scannell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "83zvY8KEo3wE"
      },
      "outputs": [],
      "source": [
        "def HMM_uni(train, test):\n",
        "    uni_tag = nltk.UnigramTagger(train)\n",
        "    acc = uni_tag.accuracy(test)\n",
        "\n",
        "    print(\"Unigram tagger accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Influenced by Dr. Scannell but largely original"
      ],
      "metadata": {
        "id": "a_zoNThk93qE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "JltwhRlCJYlb"
      },
      "outputs": [],
      "source": [
        "def gen_rule_tag():\n",
        "    patterns = [\n",
        "        (r'a .*', 'H'),\n",
        "        (r'an .*', 'H'),\n",
        "        (r'mo .*', 'H'),\n",
        "        (r'do .*', 'H'),\n",
        "        (r'don .*', 'H'),\n",
        "        (r'den .*', 'H'),\n",
        "        (r'an d.*', 'N'),\n",
        "        (r'an t.*', 'N'),\n",
        "        (r'faoi .*', 'N')\n",
        "    ]\n",
        "\n",
        "    rule_tagger = nltk.RegexpTagger(patterns)\n",
        "\n",
        "    return rule_tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFTTD2tDwCKq"
      },
      "source": [
        "Below code influenced by code provided by Dr.  and adapted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "w0oukDklv9Sb"
      },
      "outputs": [],
      "source": [
        "def HMM_bi(train, test):\n",
        "    rule_tagger = gen_rule_tag()\n",
        "\n",
        "    bigrams = gen_grams(train, 1)\n",
        "    test = gen_grams(test)\n",
        "\n",
        "    bitag = nltk.UnigramTagger(bigrams, backoff=rule_tagger)\n",
        "    acc = bitag.accuracy(test)\n",
        "\n",
        "    print(\"Bigram tagger accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below"
      ],
      "metadata": {
        "id": "11LVa7Bp9-04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "C6RuwcOd0HG0"
      },
      "outputs": [],
      "source": [
        "def HMM_tri(train, test):\n",
        "    tritag = nltk.TrigramTagger(train)\n",
        "\n",
        "    acc = tritag.accuracy(test)\n",
        "\n",
        "    print(\"Trigram tagger accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below"
      ],
      "metadata": {
        "id": "9qUL6WmK-G18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "oKokSn7QtTYW"
      },
      "outputs": [],
      "source": [
        "def sent_to_list(data):\n",
        "    all_data = []\n",
        "\n",
        "    for entry in data:\n",
        "      for tup in entry:\n",
        "        all_data.append(tup)\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below (Was also used in Challenge 1 https://github.com/GreihMurray/NLP)"
      ],
      "metadata": {
        "id": "LD0qtgCC-Ihh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "_KTfWs01HGaC"
      },
      "outputs": [],
      "source": [
        "def gen_grams(data, n = 1):\n",
        "    all_grams = []\n",
        "    \n",
        "    for sent in data:\n",
        "      cur_grams = [(' '.join([sent[i][0], sent[i+1][0]]), sent[i+1][1]) for i in range(len(sent)-1)]\n",
        "\n",
        "      cur_grams.append(sent[len(sent)-1])\n",
        "\n",
        "      all_grams.append(cur_grams)\n",
        "\n",
        "    return all_grams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below"
      ],
      "metadata": {
        "id": "sV4AO7mB-QwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "LwAszi9_rhqF"
      },
      "outputs": [],
      "source": [
        "def prep_data(data):\n",
        "    vocab, tags = pad(data)\n",
        "\n",
        "    x, y, max_len = encode(vocab, tags, data)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below"
      ],
      "metadata": {
        "id": "MRhxvGY1-Sbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "PwMoBgDQ_8kK"
      },
      "outputs": [],
      "source": [
        "def test_whole_seq(data):\n",
        "  #'/content/gdrive/MyDrive/Colab_Notebooks/NLP/fancy_seq_model.h5'\n",
        "  #'/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson_seq_model.h5'\n",
        "  #'/content/gdrive/MyDrive/Colab_Notebooks/NLP/adaGradPoisson_seq_model.h5'\n",
        "\n",
        "    model = load_seq_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson1024_seq_model.h5')\n",
        "    x_test, y_test = prep_data(data)\n",
        "\n",
        "    print('Changed:', eval_model(model, x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data():\n",
        "    with open(\"test.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "\n",
        "        cur_sent = []\n",
        "        all_sents = []\n",
        "\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            if line[0][0:2].strip() == 'N':\n",
        "                line = ('N', 'N')\n",
        "\n",
        "            if line[0] == \"<S>\":\n",
        "                if len(cur_sent) >= 1:\n",
        "                    all_sents.append(cur_sent)\n",
        "                cur_sent = []\n",
        "                continue\n",
        "            \n",
        "            if len(line) < 2:\n",
        "                print(line, i)\n",
        "\n",
        "            cur_sent.append((line[0], line[1]))\n",
        "\n",
        "    #x_test, y_test = prep_data(all_sents)\n",
        "\n",
        "    return all_sents"
      ],
      "metadata": {
        "id": "xPxi2-WnH1rv"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below except where marked"
      ],
      "metadata": {
        "id": "UCZuJ-cM-T-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "vJ1CYCjSiBee"
      },
      "outputs": [],
      "source": [
        "def anything():\n",
        "    data = read_file_to_sents()\n",
        "    test_data = read_test_data()\n",
        "\n",
        "    test_len = len(test_data)\n",
        "\n",
        "    data.extend(test_data)\n",
        "\n",
        "    print(data[:5])\n",
        "\n",
        "    print('Splitting data')\n",
        "\n",
        "  # from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    training_sentences = data[:cutoff]\n",
        "    test_sentences = data[cutoff:]\n",
        "\n",
        "  # Original\n",
        "    #all_clfs = dec_tree(X, y) # Training Decision Tree Models\n",
        "    #all_clfs = load_models() # Loading models\n",
        "    #acc_score(all_clfs, x_test, y_test) # Calculating accuracy\n",
        "\n",
        "    # HMM_uni(training_sentences, test_sentences)\n",
        "\n",
        "    # HMM_bi(training_sentences, test_sentences)\n",
        "\n",
        "    # HMM_tri(training_sentences, test_sentences)\n",
        "\n",
        "    model, x_test, y_test = seq_model(data, test_len) # Train sequential model\n",
        "\n",
        "    #test_whole_seq(data)\n",
        "\n",
        "    #x_test, y_test = read_test_data()\n",
        "\n",
        "    print(eval_model(model, x_test, y_test)) # Eval sequential model\n",
        "\n",
        "    #model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/adamPoisson1024_seq_model.h5')\n",
        "\n",
        "    del training_sentences, test_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpgqmmqiukB",
        "outputId": "45ab9983-f50c-48a7-b1ae-dc134ff9ea76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 5057059it [00:07, 668049.50it/s]\n",
            "Reading data...: 473951it [00:00, 667946.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('ansin', 'N'), (')', 'N'), ('tá', 'N'), ('níos', 'N'), ('lú', 'N'), ('gaeilge', 'N'), ('ag', 'N'), ('na', 'N'), ('gardaí', 'N'), ('ná', 'N'), ('bí', 'S'), ('ariamh', 'N'), ('ainneoin', 'N'), ('na', 'N'), ('cearta', 'U'), ('.', 'N'), ('níl', 'N'), ('sé', 'N'), ('ach', 'N'), ('roinnt', 'N'), ('seachtainí', 'N'), ('ó', 'N'), ('sin', 'S'), ('a', 'N'), ('tógadh', 'N'), ('fear', 'N'), ('bocht', 'N'), ('a', 'N'), ('tug', 'S'), ('ainm', 'N'), ('gaeilge', 'N'), ('dóibh', 'N'), ('.', 'N')], [('socraíodh', 'N'), ('go', 'N'), ('raibh', 'N'), ('gá', 'N'), ('lena', 'N'), ('leithéid', 'N'), (',', 'N'), ('mar', 'N'), ('go', 'N'), ('bíonn', 'U'), ('na', 'N')], [('tá', 'N'), ('an', 'N'), ('córas', 'N'), ('bainistíochta', 'N'), ('tar', 'N'), ('éis', 'N'), ('freastal', 'N'), ('go', 'N'), ('maith', 'N'), ('ar', 'N'), ('rialtas', 'N'), ('áitiúil', 'N'), ('na', 'N'), ('éireann', 'H'), ('agus', 'N'), ('leanfaidh', 'N'), ('údaráis', 'N'), ('áitiúla', 'N'), ('ar', 'N'), ('aghaidh', 'N'), ('ag', 'N'), ('brath', 'N'), ('ar', 'N'), ('bainisteoirí', 'S'), ('gairmiúla', 'N'), ('le', 'N'), ('treoir', 'N'), ('agus', 'N'), ('riarachán', 'N'), ('gairmiúil', 'N'), ('neamhchlaon', 'N'), ('a', 'N'), ('soláthar', 'S'), ('.', 'N')], [('john', 'N'), (\"o'donovanba\", 'N'), ('luath', 'N'), ('a', 'N'), (\"d'éirigh\", 'N'), ('larcom', 'N'), ('as', 'N'), ('a', 'N'), ('beith', 'S'), ('ag', 'N'), ('dul', 'N'), ('don', 'N'), ('gaeilge', 'S'), ('agus', 'N'), ('a', 'N'), ('roghnaigh', 'N'), ('réiteach', 'N'), ('eile', 'N'), ('áfach', 'N'), ('–', 'N'), ('fostú', 'N'), ('scoláirí', 'N'), ('gaeilge', 'N'), ('nó', 'N'), ('‘', 'N'), ('toponymic', 'N'), ('field', 'N'), ('workers', 'N'), ('’', 'N'), ('a', 'N'), ('beadh', 'S'), ('mar', 'N'), ('saothar', 'S'), ('acu', 'N'), ('cun', 'S'), ('an', 'N'), ('bunchiall', 'N'), ('teangeolaíochta', 'N'), ('le', 'N'), ('logainmneacha', 'N'), ('a', 'N'), ('bunú', 'S'), ('is', 'N'), ('a', 'N'), ('socrú', 'S'), ('.', 'N')], [('teangeolaí', 'N'), ('agus', 'N'), ('tráchtaire', 'N'), ('polaitiúil', 'N'), ('den', 'N'), ('céad', 'S'), ('scoth', 'N'), (',', 'N'), ('seo', 'N'), ('fear', 'N'), ('gur', 'N'), ('fiú', 'N'), ('éisteacht', 'N'), ('leis', 'N'), ('.', 'N')]]\n",
            "Splitting data\n",
            "122087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/50\n",
            "167/167 [==============================] - 64s 361ms/step - loss: 0.2303 - accuracy: 0.8961 - val_loss: 0.2011 - val_accuracy: 0.9411\n",
            "Epoch 2/50\n",
            "167/167 [==============================] - 60s 358ms/step - loss: 0.1916 - accuracy: 0.9519 - val_loss: 0.1860 - val_accuracy: 0.9605\n",
            "Epoch 3/50\n",
            "167/167 [==============================] - 60s 358ms/step - loss: 0.1822 - accuracy: 0.9707 - val_loss: 0.1794 - val_accuracy: 0.9769\n",
            "Epoch 4/50\n",
            "167/167 [==============================] - 59s 356ms/step - loss: 0.1775 - accuracy: 0.9795 - val_loss: 0.1767 - val_accuracy: 0.9806\n",
            "Epoch 5/50\n",
            "167/167 [==============================] - 59s 355ms/step - loss: 0.1754 - accuracy: 0.9830 - val_loss: 0.1754 - val_accuracy: 0.9830\n",
            "Epoch 6/50\n",
            "167/167 [==============================] - 60s 357ms/step - loss: 0.1741 - accuracy: 0.9853 - val_loss: 0.1747 - val_accuracy: 0.9844\n",
            "Epoch 7/50\n",
            "167/167 [==============================] - 59s 354ms/step - loss: 0.1734 - accuracy: 0.9868 - val_loss: 0.1742 - val_accuracy: 0.9855\n",
            "Epoch 8/50\n",
            "167/167 [==============================] - 59s 352ms/step - loss: 0.1728 - accuracy: 0.9880 - val_loss: 0.1739 - val_accuracy: 0.9862\n",
            "Epoch 9/50\n",
            "167/167 [==============================] - 59s 353ms/step - loss: 0.1724 - accuracy: 0.9889 - val_loss: 0.1737 - val_accuracy: 0.9868\n",
            "Epoch 10/50\n",
            "167/167 [==============================] - 59s 355ms/step - loss: 0.1720 - accuracy: 0.9897 - val_loss: 0.1735 - val_accuracy: 0.9873\n",
            "Epoch 11/50\n",
            "167/167 [==============================] - 59s 353ms/step - loss: 0.1717 - accuracy: 0.9903 - val_loss: 0.1734 - val_accuracy: 0.9877\n",
            "Epoch 12/50\n",
            "167/167 [==============================] - 59s 353ms/step - loss: 0.1715 - accuracy: 0.9908 - val_loss: 0.1732 - val_accuracy: 0.9880\n",
            "Epoch 13/50\n",
            "167/167 [==============================] - 59s 351ms/step - loss: 0.1712 - accuracy: 0.9913 - val_loss: 0.1731 - val_accuracy: 0.9883\n",
            "Epoch 14/50\n",
            "167/167 [==============================] - 59s 352ms/step - loss: 0.1710 - accuracy: 0.9917 - val_loss: 0.1730 - val_accuracy: 0.9884\n",
            "Epoch 15/50\n",
            "167/167 [==============================] - 59s 355ms/step - loss: 0.1708 - accuracy: 0.9921 - val_loss: 0.1730 - val_accuracy: 0.9887\n",
            "Epoch 16/50\n",
            "167/167 [==============================] - 59s 352ms/step - loss: 0.1706 - accuracy: 0.9925 - val_loss: 0.1729 - val_accuracy: 0.9888\n",
            "Epoch 17/50\n",
            "167/167 [==============================] - 58s 350ms/step - loss: 0.1704 - accuracy: 0.9929 - val_loss: 0.1729 - val_accuracy: 0.9887\n",
            "Epoch 18/50\n",
            "167/167 [==============================] - 58s 350ms/step - loss: 0.1702 - accuracy: 0.9932 - val_loss: 0.1729 - val_accuracy: 0.9889\n",
            "Epoch 19/50\n",
            "167/167 [==============================] - 58s 350ms/step - loss: 0.1701 - accuracy: 0.9935 - val_loss: 0.1729 - val_accuracy: 0.9890\n",
            "Epoch 20/50\n",
            "167/167 [==============================] - 59s 355ms/step - loss: 0.1699 - accuracy: 0.9937 - val_loss: 0.1729 - val_accuracy: 0.9889\n",
            "Epoch 21/50\n",
            "167/167 [==============================] - 59s 351ms/step - loss: 0.1698 - accuracy: 0.9940 - val_loss: 0.1729 - val_accuracy: 0.9890\n",
            "Epoch 22/50\n",
            "167/167 [==============================] - 59s 351ms/step - loss: 0.1697 - accuracy: 0.9943 - val_loss: 0.1730 - val_accuracy: 0.9889\n",
            "Epoch 23/50\n",
            "167/167 [==============================] - 59s 352ms/step - loss: 0.1696 - accuracy: 0.9945 - val_loss: 0.1731 - val_accuracy: 0.9888\n",
            "Epoch 24/50\n",
            "167/167 [==============================] - 59s 353ms/step - loss: 0.1695 - accuracy: 0.9947 - val_loss: 0.1732 - val_accuracy: 0.9889\n",
            "587/587 [==============================] - 20s 33ms/step - loss: 0.1731 - accuracy: 0.9890\n",
            "{'loss': 0.1730583906173706, 'accuracy': 0.9890205264091492}\n"
          ]
        }
      ],
      "source": [
        "anything()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2bEug_WMoQf"
      },
      "source": [
        "ADAM Poisson Accuracy: 99.371\n",
        "\n",
        "ADAM Cross Entropy Accuracy: 99.370\n",
        "\n",
        "ADAGRAD Poisson Accuracy: 94.01\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMcmHupKY1HEBPXoDKtF/Iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}