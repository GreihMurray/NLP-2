{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPJ8Dld4Rj/llLn1mkw3REa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-2/blob/AG_Murray/irish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sPoargROh-CD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJvphPGY7b3",
        "outputId": "54d4b70d-b440-48ea-d8de-e5cdd977bad7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_to_sents():\n",
        "    with open(\"train.tsv\", encoding=\"utf-8\") as file:\n",
        "        f = csv.reader(file, delimiter=\"\\t\")\n",
        "\n",
        "        cur_sent = []\n",
        "        all_sents = []\n",
        "\n",
        "        for line in tqdm(f, desc=\"Reading data...\"):\n",
        "            if line[0][0:2].strip() == 'N':\n",
        "                line[0] = 'N'\n",
        "\n",
        "            if line[0] == \"<S>\":\n",
        "                if len(cur_sent) >= 1:\n",
        "                    all_sents.append(cur_sent)\n",
        "                cur_sent = []\n",
        "                continue\n",
        "\n",
        "            cur_sent.append((line[0], line[1]))\n",
        "\n",
        "    return all_sents"
      ],
      "metadata": {
        "id": "uNyCz3x0i2DH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_num(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    label = le.fit_transform(data)\n",
        "\n",
        "    return label"
      ],
      "metadata": {
        "id": "hyVCOhZzsj7P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def defeature(data):\n",
        "    unfeatured = []\n",
        "\n",
        "    for dict in data:\n",
        "        unfeatured.append(dict['word'])\n",
        "\n",
        "    return unfeatured"
      ],
      "metadata": {
        "id": "brkGW1HtsmZ1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xdUPk8WGiX36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_model(x, y):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(1,), activation=\"sigmoid\"))\n",
        "    model.add(Dense(128, activation=\"sigmoid\"))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    model.add(Dense(1, activation=\"softmax\"))\n",
        "\n",
        "  # From https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n",
        "    #stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    x = defeature(x)\n",
        "\n",
        "    x = string_to_num(x)\n",
        "    y = string_to_num(y)\n",
        "\n",
        "    x = np.asarray(x)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    print(\"[INFO] training network...\")\n",
        "    sgd = SGD(0.01)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
        "    H = model.fit(x[:10000], y[:10000], epochs=100, batch_size=100) #, callbacks=[stop_early])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vCoRohe7iDg5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "  x_test = defeature(x_test)\n",
        "  x_test = string_to_num(x_test)\n",
        "  y_test = string_to_num(y_test)\n",
        "  return model.evaluate(x_test[:5000], y_test[:5000], return_dict=True)"
      ],
      "metadata": {
        "id": "jPe1yG7k3UlV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below work partially https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "H2fNKCqniaML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dec_tree(X, y):\n",
        "    clf = Pipeline([\n",
        "        ('vectorizer', DictVectorizer(sparse=False)),\n",
        "        ('classifier', DecisionTreeClassifier(criterion='entropy')),\n",
        "    ], verbose=1)\n",
        "\n",
        "    print(\"Training Started\")\n",
        "\n",
        "    # Custom work below\n",
        "    all_clfs = []\n",
        "\n",
        "    for i in tqdm(range(0, 115), desc=\"Training\"):\n",
        "        cur_clf = clf\n",
        "        cur_clf.fit(X[(i * 31649): ((i + 1) * 31649)], y[(i * 31649): ((i + 1) * 31649)])  # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
        "\n",
        "        all_clfs.append(cur_clf)\n",
        "\n",
        "        file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "        file_loc_2 = '/gdrive/MyDrive/Colab_Notebooks/NLP/vector' + str(i) + \".pkl\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "        with open(file_loc, 'wb') as pickle_file:\n",
        "          pickle.dump(cur_clf, pickle_file)\n",
        "\n",
        "        # cur_clf.named_steps['estimator'].model = None\n",
        "\n",
        "        # joblib.dump(cur_clf, file_loc_2)\n",
        "\n",
        "    print('Training completed')\n",
        "\n",
        "    X_test, y_test = transform_to_dataset(test_sentences)\n",
        "\n",
        "    return all_clfs"
      ],
      "metadata": {
        "id": "8Fk2d3XkiITc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4KX-6M76iggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_score(all_clfs, x_test, y_test):\n",
        "    all_scores = []\n",
        "\n",
        "    for clf in tqdm(all_clfs, desc=\"evaluating...\"):\n",
        "        all_scores.append(clf.score(x_test[:50000], y_test[:50000]))\n",
        "\n",
        "    total_acc = 0\n",
        "\n",
        "    for score in all_scores:\n",
        "        total_acc += score\n",
        "\n",
        "    print(\"Accuracy:\", total_acc / len(all_scores))"
      ],
      "metadata": {
        "id": "X3xUNpkIiKnW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "r3j2axkJiiPS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "    }"
      ],
      "metadata": {
        "id": "zt5W_NP_iNRE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "LYe6tgA6ijKY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def untag(tagged_sentence):\n",
        "    return [w for w, t in tagged_sentence]"
      ],
      "metadata": {
        "id": "AbjAsK4biPi8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below work comes from https://nlpforhackers.io/training-pos-tagger/amp/"
      ],
      "metadata": {
        "id": "ZzzBYcRJikGI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_dataset(tagged_sentences):\n",
        "    X, y = [], []\n",
        "\n",
        "    for tagged in tagged_sentences:\n",
        "        for index in range(len(tagged)):\n",
        "            X.append(features(untag(tagged), index))\n",
        "            y.append(tagged[index][1])\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "IhqgzoFyiSEg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "  all_clfs = []\n",
        "\n",
        "  for i in range(1, 115):\n",
        "    file_loc = '/content/gdrive/MyDrive/Colab_Notebooks/NLP/model' + str(i) + \".h5\"\n",
        "\n",
        "        # cur_clf.named_steps['classifier'].save(file_loc)\n",
        "\n",
        "    with open(file_loc, 'rb') as pickle_file:\n",
        "      cur_clf = pickle.load(pickle_file)\n",
        "\n",
        "      all_clfs.append(cur_clf)\n",
        "\n",
        "  return all_clfs\n",
        "\n"
      ],
      "metadata": {
        "id": "_i6pbwlXun9i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anything():\n",
        "    data = read_file_to_sents()\n",
        "\n",
        "  # from https://nlpforhackers.io/training-pos-tagger/amp/\n",
        "    cutoff = int(.75 * len(data))\n",
        "    training_sentences = data[:cutoff]\n",
        "    test_sentences = data[cutoff:]\n",
        "\n",
        "    X, y = transform_to_dataset(training_sentences)\n",
        "\n",
        "    del data, training_sentences\n",
        "\n",
        "    x_test, y_test = transform_to_dataset(test_sentences)\n",
        "\n",
        "  # Original\n",
        "    #all_clfs = dec_tree(X, y)\n",
        "    all_clfs = load_models()\n",
        "    acc_score(all_clfs, x_test, y_test)\n",
        "\n",
        "    #model = seq_model(X, y)\n",
        "    #print(eval_model(model, x_test, y_test))"
      ],
      "metadata": {
        "id": "vJ1CYCjSiBee"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anything()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpgqmmqiukB",
        "outputId": "d4574d08-9e06-42cb-824d-32b5045e74f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data...: 5057059it [00:06, 783986.25it/s]\n",
            "evaluating...:  20%|██        | 23/114 [02:21<08:22,  5.52s/it]"
          ]
        }
      ]
    }
  ]
}